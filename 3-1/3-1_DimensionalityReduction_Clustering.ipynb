{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 次元削減とクラスタリング\n",
    "\n",
    "# 0. 事前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.width', 800)\n",
    "\n",
    "import matplotlib\n",
    "import sklearn\n",
    "print('pandas',pd.__version__)             # VM+Ubuntu: 0.23.4\n",
    "print('numpy',np.__version__)              # VM+Ubuntu: 1.15.1\n",
    "print('matplotlib',matplotlib.__version__) # VM+Ubuntu: 2.2.3\n",
    "print('seaborn',sns.__version__)           # VM+Ubuntu: 0.9.0\n",
    "print('scikit-learn',sklearn.__version__)  # VM+Ubuntu: 0.19.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "df = pd.read_table('./input/count_tpm.tsv', index_col=0)\n",
    "print(df.head())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 全サンプルでTPMがゼロの遺伝子のレコードを削除\n",
    "all_zero_index = df.index[df.sum(axis=1) == 0]\n",
    "df = df.drop(all_zero_index)\n",
    "print(df.head())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# サンプルをプロットするときの色を設定\n",
    "sample_colors = {'batch_1':'b',  # b : blue\n",
    "                 'batch_2':'b',\n",
    "                 'batch_3':'b',\n",
    "                 'chemostat_1':'g', # g: green\n",
    "                 'chemostat_2':'g',\n",
    "                 'chemostat_3':'g'}\n",
    "colors = df.columns.map(sample_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 次元削減"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 1 行列分解に基づく次元削減"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 PCA (Principal Component Analysis; 主成分分析)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PCA実行\n",
    "pca = sklearn.decomposition.PCA()\n",
    "coords = pca.fit_transform(df.transpose().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scatter_plot(coords, sample_labels, colors, xlabel=None, ylabel=None, title=''):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(coords[:, 0], coords[:, 1], color=colors)\n",
    "    for i, sample_label in enumerate(sample_labels):\n",
    "        ax.annotate(sample_label, xy=(coords[i, :2]), xytext=(10,10),\n",
    "                    textcoords='offset points', color=colors[i],\n",
    "                   arrowprops={'arrowstyle':'-', 'edgecolor':colors[i]})\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scatter_plot(coords, df.columns, colors, xlabel='PC1', ylabel='PC2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# z-score 標準化\n",
    "import sklearn.preprocessing\n",
    "values = df.transpose().values\n",
    "scaler = sklearn.preprocessing.StandardScaler(with_mean=True, with_std=True)\n",
    "std_values = scaler.fit_transform(values)\n",
    "std_df = pd.DataFrame(std_values.T, index=df.index, columns=df.columns)\n",
    "\n",
    "print('\\nRaw values')\n",
    "print('TPM Average :')\n",
    "print(df.mean(axis=1)[:10])\n",
    "print('\\nTPM Standard deviation :')\n",
    "print(df.std(ddof=0, axis=1)[:10])\n",
    "\n",
    "print('\\n\\nStandardized values')\n",
    "print('TPM Average :')\n",
    "print(std_df.mean(axis=1)[:10])\n",
    "print('\\nTPM Standard deviation :')\n",
    "print(std_df.std(ddof=0, axis=1)[:10])\n",
    "\n",
    "#おんなじことを自分で計算する場合\n",
    "#values = df.transpose().values\n",
    "#std_values = (values - values.mean(axis=0)) / values.std(axis=0)\n",
    "#std_df2 = pd.DataFrame(std_values.T, index=df.index, columns=df.columns)\n",
    "#print('\\n\\nStandardized values 2')\n",
    "#print('TPM Average :')\n",
    "#print(std_df2.mean(axis=1)[:10])\n",
    "#print('\\nTPM Standard deviation:')\n",
    "#print(std_df2.std(ddof=0, axis=1)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 標準化されたデータでPCA実行\n",
    "pca = sklearn.decomposition.PCA()\n",
    "coords = pca.fit_transform(std_df.transpose().values)\n",
    "scatter_plot(coords, std_df.columns, colors, xlabel='PC1', ylabel='PC2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 各主成分の「寄与率」\n",
    "print(['{:.2f}%'.format(x*100) for x in pca.explained_variance_ratio_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 NMF (Non-negative Matrix Factorization; 非負値行列因子分解)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X ~ W * H\n",
    "\n",
    "\n",
    "X: non-negative matrix. (n, m)\n",
    "\n",
    "W: non-negative matrix. (n, k)\n",
    "\n",
    "H: non-negative matrix. (k, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 擬似データでNMFのデモ\n",
    "\n",
    "def plot_W_H_X(W, H, X):\n",
    "    classes = ['Class%d'%x for x in range(W.shape[1])]\n",
    "    genes = ['Gene%d'%x for x in range(W.shape[0])]\n",
    "    samples = ['Sample%d'%x for x in range(H.shape[1])]\n",
    "    fig = plt.figure()\n",
    "    ax1 = plt.axes([0.0, 0.1, 0.1, 0.8])\n",
    "    sns.heatmap(pd.DataFrame(W, index=genes, columns=classes), cmap='Greys', cbar=False, ax=ax1)\n",
    "    ax2 = plt.axes([0.2, 0.3, 0.4, 0.5])\n",
    "    pd.DataFrame(H, index=classes, columns=samples).transpose().plot.bar(stacked=True, ax=ax2)\n",
    "    plt.legend(loc=(0., 1.04), ncol=3, fontsize=8)\n",
    "    ax3 = plt.axes([0.8, 0.1, 0.4, 0.8])\n",
    "    sns.heatmap(pd.DataFrame(X, index=genes, columns=samples), cmap='Greys', cbar=False)\n",
    "    plt.show()\n",
    "\n",
    "print('Original')\n",
    "                   # class-0 は最初の3個の遺伝子を発現\n",
    "genes = np.array([[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n",
    "                  # class-1 はまんなか3個の遺伝子を発現\n",
    "                   [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                  # class-2 はうしろ4個の遺伝子を発現\n",
    "                   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]]).T\n",
    "# 各サンプルがclass-0, class-1, class2をどういう割合で持っているか\n",
    "samples = np.random.dirichlet(alpha=[1.0]*3, size=10).T\n",
    "# 遺伝子発現テーブルはその掛け算で決まっている（と仮定する）\n",
    "original_expression_data = np.dot(genes, samples)\n",
    "plot_W_H_X(genes, samples, original_expression_data)\n",
    "\n",
    "print('Reconstructed')\n",
    "# 遺伝子発現テーブルだけを使って、クラスごとの発現パターンベクトル、サンプルごとのクラス割合を復元する\n",
    "model = sklearn.decomposition.NMF(n_components=3)\n",
    "W = model.fit_transform(original_expression_data)\n",
    "W /= W.sum(axis=0)\n",
    "H = model.components_\n",
    "H /= H.sum(axis=0)\n",
    "X = np.dot(W, H)\n",
    "plot_W_H_X(W, H, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = sklearn.decomposition.NMF(n_components=2)\n",
    "W = model.fit_transform(df.values)\n",
    "H = model.components_\n",
    "\n",
    "print('Original shape=',df.values.shape)\n",
    "print('W shape =',W.shape)\n",
    "print('H shape =',H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 次元削減としての利用\n",
    "H_percentage = 100.0 * H / H.sum(axis=0)\n",
    "pd.DataFrame(H_percentage.T, index=df.columns, columns=['Class1', 'Class2']).plot.bar(stacked=True)\n",
    "#各サンプル、もっとも値の高い要素に割り当てることでクラスタリングの代わりとしてしまうこともある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# それぞれの因子に強く寄与している遺伝子はなにか？\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(W[:,0], W[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 遺伝子のproduct情報をロード\n",
    "gene_products = pd.read_table('./input/gene_id_product.tsv', index_col=0, names=[\"gene_id\", \"product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 因子１　に強く寄与する遺伝子\n",
    "topN = 50\n",
    "top_factor1 = df.index[ np.argsort(W[:,0] - W[:,1])[::-1][:topN] ]\n",
    "gene_labels = top_factor1 + ' --- ' + gene_products.loc[top_factor1, 'product']\n",
    "\n",
    "fig = plt.figure(figsize=(9,16))\n",
    "sns.heatmap(df.loc[top_factor1, :], yticklabels=gene_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 因子2　に強く寄与する遺伝子\n",
    "top_factor2 = df.index[ np.argsort(W[:,0] - W[:,1])[:topN] ]\n",
    "gene_labels = top_factor2 + ' --- ' + gene_products.loc[top_factor2, 'product']\n",
    "\n",
    "fig = plt.figure(figsize=(9,16))\n",
    "sns.heatmap(df.loc[top_factor2, :], yticklabels=gene_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 LSI (Latent semantic indexing; 潜在意味解析)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF 変換\n",
    "# TF ... Term Frequency   いくつかの流儀がある。ここではサンプルごとのmaxに対する割合\n",
    "TF = df.values / df.values.max(axis=0)\n",
    "# IDF ... Inverse Document Frequency  これもいくつかの流儀あり。\n",
    "n_samples = len(df.columns)\n",
    "IDF = np.log2(1.0 + (float(n_samples) / df.values.astype(bool).sum(axis=1)))\n",
    "# TF-IDF\n",
    "TFIDF = TF * IDF[:, np.newaxis]\n",
    "df_tfidf = pd.DataFrame(TFIDF, index=df.index, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SVD (Singular Value Decomposition; 特異値分解)を実行\n",
    "import numpy.linalg\n",
    "U, Sig, V = np.linalg.svd(df_tfidf.values, full_matrices=False)\n",
    "Weights = np.dot(np.diag(Sig), V)\n",
    "\n",
    "# U, Weights は元の行列を分解したものなので、UとWeightsの掛け算は元の行列を近似\n",
    "np.allclose(df_tfidf.values, np.dot(U, Weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U_reduced = U[:, :2]\n",
    "W_reduced = np.dot(np.diag(Sig[:2]), V[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121)\n",
    "sns.heatmap(df_tfidf.values[:200,:], cbar=False, ax=ax1)\n",
    "ax1.set_title('Original')\n",
    "ax2 = fig.add_subplot(122)\n",
    "sns.heatmap(np.dot(U_reduced, W_reduced)[:200,:], cbar=False, ax=ax2)\n",
    "ax2.set_title('Reconstructed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 長さ１のベクトルにノーマライズする\n",
    "W_norm = W_reduced / np.sqrt((W_reduced**2).sum(axis=0))\n",
    "# 2つのcomponentの「重み」空間でサンプルをプロット\n",
    "scatter_plot(W_norm.T, df_tfidf.columns, colors, xlabel='Component-1', ylabel='Component-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# batch と chemostatの違いは、　Component-2がプラスに寄与するかマイナスに寄与するか。\n",
    "# Components-2にどういう遺伝子の合成ベクトルなのか、係数が大きい遺伝子をいくつか見てみる\n",
    "C2_genes = pd.DataFrame(U_reduced[:, 1], index=df_tfidf.index, columns=['weights'])\n",
    "# 係数の絶対値が大きいトップ50の遺伝子を取得\n",
    "topN = 50\n",
    "C2_genes['abs weights'] = np.abs(C2_genes['weights'].values)\n",
    "C2_genes = C2_genes.sort_values(by=['abs weights'], ascending=False)\n",
    "C2_genes = C2_genes.head(topN)\n",
    "C2_genes = C2_genes.sort_values(by=['weights'])\n",
    "C2_genes['labels'] = C2_genes.index + ' --- ' + gene_products.loc[C2_genes.index, 'product']\n",
    "C2_genes.plot.barh(y='weights', x='labels', figsize=(9, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 2 距離行列の最適化に基づく次元削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#非線形次元圧縮、各手法の比較\n",
    "#他の手法は以下のURLを参照\n",
    "#https://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html\n",
    "\n",
    "from time import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from sklearn import manifold, datasets\n",
    "Axes3D\n",
    "\n",
    "n_points = 1000\n",
    "X, color = datasets.samples_generator.make_s_curve(n_points, random_state=0)\n",
    "n_neighbors = 10\n",
    "n_components = 2\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "plt.suptitle(\"Manifold Learning with %i points, %i neighbors\" % (1000, n_neighbors), fontsize=14)\n",
    "\n",
    "ax = fig.add_subplot(141, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "ax.view_init(4, -72)\n",
    "\n",
    "t0 = time()\n",
    "pca = sklearn.decomposition.PCA()\n",
    "Y = pca.fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"PCA: %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(142)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"PCA (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "\n",
    "t0 = time()\n",
    "mds = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
    "Y = mds.fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"MDS: %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(143)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"MDS (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "\n",
    "t0 = time()\n",
    "tsne = manifold.TSNE(n_components=n_components, init='pca', random_state=0)\n",
    "Y = tsne.fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"t-SNE: %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(144)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"t-SNE (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 MDS (Multidimensional scaling; 多次元尺度構成法)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metric MDS = PCoA (Principal Coordinate Analysis; 主座標分析)\n",
    "構成する「距離行列」が、距離関数の要件を満たしている場合に適用可能な手法。\n",
    "\n",
    "Bray-Curtis dissimilarityなど、三角不等式の要件を満たさない非類似性指標があるので注意。こういう場合はPCoAではなくnMDSを使う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# デフォルトではユークリッド距離でサンプル間距離行列を計算。この場合、数学的には主成分分析と等価。\n",
    "# ただしscikit-learnのMDS実装はiterativeに最適化するmetric MDSであるため（classical MDS=PCoAは固有値分解に基づく手法）、\n",
    "# ランダムな初期値の影響で実行のたびに結果が若干変わる\n",
    "mds = sklearn.manifold.MDS(n_components=2, dissimilarity='euclidean')\n",
    "coords = mds.fit_transform(std_df.transpose().values)\n",
    "scatter_plot(coords, std_df.columns, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 距離行列の計算\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "distance_matrix = squareform(pdist(std_df.transpose().values))\n",
    "print(distance_matrix)\n",
    "\n",
    "#自分で距離行列を作る場合（上の一行と同じ計算結果）\n",
    "#from scipy.spatial.distance import euclidean\n",
    "#values = std_df.transpose().values\n",
    "#distance_matrix_2 = []\n",
    "#for i in range(values.shape[0]):\n",
    "#    vec = []\n",
    "#    for j in range(values.shape[0]):\n",
    "#        vec.append(euclidean(values[i, :], values[j, :]))\n",
    "#    distance_matrix_2.append(vec)\n",
    "#print(np.array(distance_matrix_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mds = sklearn.manifold.MDS(n_components=2, dissimilarity='precomputed')\n",
    "coords = mds.fit_transform(distance_matrix)\n",
    "scatter_plot(coords, std_df.columns, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 相関係数で計算した距離（1 - 相関係数）\n",
    "distance_matrix = squareform(pdist(std_df.transpose().values, 'correlation'))\n",
    "mds = sklearn.manifold.MDS(n_components=2, dissimilarity='precomputed')\n",
    "coords = mds.fit_transform(distance_matrix)\n",
    "scatter_plot(coords, std_df.columns, colors, title='Correlation')\n",
    "# Jaccard距離\n",
    "distance_matrix = squareform(pdist(std_df.transpose().values, 'jaccard'))\n",
    "mds = sklearn.manifold.MDS(n_components=2, dissimilarity='precomputed')\n",
    "coords = mds.fit_transform(distance_matrix)\n",
    "scatter_plot(coords, std_df.columns, colors, title='Jaccard distance')\n",
    "# マンハッタン距離\n",
    "distance_matrix = squareform(pdist(std_df.transpose().values, 'cityblock'))\n",
    "mds = sklearn.manifold.MDS(n_components=2, dissimilarity='precomputed')\n",
    "coords = mds.fit_transform(distance_matrix)\n",
    "scatter_plot(coords, std_df.columns, colors, title='Manhattan distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-metric MDS (非計量多次元尺度構成法)\n",
    "構成する「距離行列」が、距離関数の要件を満たさない場合に適用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bray-Curtis 非類似度指標による計算\n",
    "distance_matrix = squareform(pdist(std_df.transpose().values, 'braycurtis'))\n",
    "nmds = sklearn.manifold.MDS(n_components=2, metric=False, dissimilarity='precomputed')\n",
    "coords = nmds.fit_transform(distance_matrix)\n",
    "scatter_plot(coords, std_df.columns, colors, title='Bray-Curtis dissimilarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 多様体学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE (t-distributed Stochastic Neighbor Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsne = sklearn.manifold.TSNE(n_components=2)\n",
    "coords = tsne.fit_transform(std_df.transpose().values)\n",
    "scatter_plot(coords, std_df.columns, colors, title='t-SNE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. クラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 階層的クラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.clustermap(df, method='average', metric='correlation', col_colors=colors, figsize=(3, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_transform = lambda x: np.log10(x + 1.0)\n",
    "sns.clustermap(df.apply(log_transform), method='average', metric='correlation', col_colors=colors, figsize=(3, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 平均TPMのサンプル間分散がTop-20の遺伝子だけ抜き出し\n",
    "top20_df = df.loc[df.var(axis=1).sort_values(ascending=False).index[:20], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# サンプル間の距離計算手法、クラスター間の距離計算手法による違い\n",
    "sns.clustermap(top20_df, method='average', metric='correlation', col_colors=colors)\n",
    "sns.clustermap(top20_df, method='ward', metric='euclidean', col_colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 K-means & K-medoids （クラスタ「中心」との距離に基づくクラスタリング）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "model = sklearn.cluster.KMeans(n_clusters=2)\n",
    "y = model.fit_predict(df.transpose().values)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# silhouette解析による適切なクラスター数の推定\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "X = df.transpose().values\n",
    "pca = sklearn.decomposition.PCA()\n",
    "pca_coords = pca.fit_transform(X)\n",
    "\n",
    "for n_clusters in [2, 3, 4]:\n",
    "    model = sklearn.cluster.KMeans(n_clusters=n_clusters)\n",
    "    y = model.fit_predict(X)\n",
    "    silhouette_avg = silhouette_score(X, y)\n",
    "    title = 'n_clusters: {}\\nAverage silhouette score = {:.2f}'.format(n_clusters, silhouette_avg)\n",
    "    cluster_colors = cm.nipy_spectral(y.astype(float) / n_clusters)\n",
    "    scatter_plot(pca_coords, std_df.columns, cluster_colors, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# もっとサンプル数がたくさんある場合は、サンプルごとのシルエットスコアの分布を比較できる\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "X, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1, center_box=(-10.0, 10.0), shuffle=True, random_state=1)\n",
    "\n",
    "def plot_silhouette(n_clusters):\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    model = sklearn.cluster.KMeans(n_clusters=n_clusters)\n",
    "    cluster_labels = model.fit_predict(X)\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print('n_clusters:',n_clusters,' Average silhouette score =',silhouette_avg)\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = np.sort(sample_silhouette_values[cluster_labels == i])\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        cluster_colors = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=cluster_colors, alpha=0.7)\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    cluster_colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], c=cluster_colors)\n",
    "    plt.show()\n",
    "\n",
    "for n_clusters in [2, 3, 4]:\n",
    "    plot_silhouette(n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-medoids clustering\n",
    "\n",
    "平均（mean）ではなくmedoid（すべてのデータ点との距離の和が最小になるデータ点）を使う点だけがK-meansと異なる。\n",
    "\n",
    "実データの平均を計算する必要がないため、実データを入力する必要がなく、距離行列があればいい\n",
    "\n",
    "→ ユークリッド距離である必要はなく、任意の距離関数を使える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# K-medoids clusteringをscikit-learnの使用感っぽく使えるように実装した。\n",
    "# エラーハンドリングとか全然してないので注意。また、初期値依存性も高い。sklearnのKMeansメソッドは初期化を工夫している。\n",
    "# 参考：https://www.researchgate.net/publication/272351873_NumPy_SciPy_Recipes_for_Data_Science_k-Medoids_Clustering\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "class KMedoids():\n",
    "    \"\"\"K-Medoids clustering\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_clusters : int, optional, default: 2\n",
    "        Number of clusters to form.\n",
    "    max_iter : int, optional, default: 100\n",
    "        Maximum number of iterations of the k-medoids algorithm.\n",
    "    verbose : boolean, optional, default: False\n",
    "        Verbosity mode.\n",
    "    random_state : int, RandomState instance or None, optional, default: None\n",
    "    dissimilarity : 'euclidean' | 'precomputed', optional, default: 'euclidean'\n",
    "        Dissimilarity measure to use:\n",
    "            - 'euclidean':\n",
    "                Pairwise Euclidean distances between points in the dataset.\n",
    "            - 'precomputed':\n",
    "                Pre-computed dissimilarities are passed.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    cluster_medoids_ : array, [n_clusters]\n",
    "        Indices of cluster medoids\n",
    "    labels_ :\n",
    "        Labels of each point\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=2, max_iter=100, verbose=False,\n",
    "                random_state=None, dissimilarity='euclidean'):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.dissimilarity = dissimilarity\n",
    "    \n",
    "    def _init_medoids(self, X, n_clusters, random_state):\n",
    "        n_samples = X.shape[0]\n",
    "        init_indices = random_state.choice(n_samples, n_clusters, replace=False)\n",
    "        return np.sort(init_indices)\n",
    "    \n",
    "    def _k_medoids(self, X, n_clusters, max_iter, random_state, verbose):\n",
    "        # 初期medoidsをランダムにn_clusters個選択。\n",
    "        medoids = self._init_medoids(X, n_clusters, random_state)\n",
    "        if self.verbose:\n",
    "            print('initial medoids =', medoids)\n",
    "        new_medoids = np.copy(medoids)\n",
    "        # ループ開始。medoidsが収束するまで繰り返す。\n",
    "        for i in range(max_iter):\n",
    "            # 各サンプルをもっとも近いmedoidsに割り当てる。\n",
    "            assigned_cluster_labels = np.argmin(X[:, medoids], axis=1)\n",
    "            for c in range(n_clusters): # クラスタごとに新たなmedoidsを選択。\n",
    "                # クラスタc に割り当てられたサンプルのインデックスを取得。\n",
    "                sample_indices_in_c = np.where(assigned_cluster_labels == c)[0]\n",
    "                # クラスタc に割り当てられたサンプル内の距離行列を抽出。\n",
    "                distance_matrix_of_c = X[np.ix_(sample_indices_in_c, sample_indices_in_c)]\n",
    "                # クラスタc 内の各サンプルについて、同一クラスタ内サンプルとの平均距離を計算\n",
    "                average_distance_in_c = np.mean(distance_matrix_of_c, axis=1)\n",
    "                # 新たなmedoidは平均距離がもっとも小さいサンプル。\n",
    "                new_medoid_index_in_c = np.argmin(average_distance_in_c)\n",
    "                new_medoid_index = sample_indices_in_c[new_medoid_index_in_c]\n",
    "                new_medoids[c] = new_medoid_index\n",
    "            np.sort(new_medoids)\n",
    "            if self.verbose:\n",
    "                print('\\titeration:',i,'\\n\\t\\tnew medoids=',new_medoids)\n",
    "            if np.array_equal(new_medoids, medoids):\n",
    "                # medoidsのインデックスが更新されなかったらループ終了。\n",
    "                break\n",
    "            medoids = np.copy(new_medoids)\n",
    "        assigned_cluster_labels = np.argmin(X[:, medoids], axis=1)\n",
    "        return medoids, assigned_cluster_labels\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Compute k-medoids clustering.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features) or (n_samples, n_samples)\n",
    "            Input data. If ``dissimilarity == 'precomputed'``, the input should\n",
    "            be the dissimilarity matrix.\n",
    "        \"\"\"\n",
    "        if self.dissimilarity == 'precomputed':\n",
    "            self.dissimilarity_matrix_ = X\n",
    "        elif self.dissimilarity == 'euclidean':\n",
    "            self.dissimilarity_matrix_ = squareform(pdist(X))\n",
    "        else:\n",
    "            raise ValueError(\"dissimilarity must be 'precomputed' or 'euclidean'.\")\n",
    "        if self.random_state is None or self.random_state is np.random:\n",
    "            self.random_state = np.random.mtrand._rand\n",
    "        elif isinstance(self.random_state, int):\n",
    "            self.random_state = np.random.RandomState(self.random_state)\n",
    "        elif isinstance(self.random_state, np.random.RandomState):\n",
    "            self.random_state = np.random.RandomState\n",
    "        else:\n",
    "            raise ValueError(\"%r cannot be used to seed a numpy.random.RandomState\" % self.random_state)\n",
    "        self.cluster_medoids_, self.labels_ = \\\n",
    "            self._k_medoids(self.dissimilarity_matrix_, self.n_clusters, self.max_iter, self.random_state, self.verbose)\n",
    "        return self\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"Compute k-medoids clustering and predict cluster index for each sample.\n",
    "        \"\"\"\n",
    "        return self.fit(X).labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# K-medoidsクラスタリングの実行\n",
    "distance_matrix = squareform(pdist(df.transpose().values, metric='correlation')) # 相関係数距離行列\n",
    "n_clusters = 2\n",
    "\n",
    "model = KMedoids(n_clusters=n_clusters, dissimilarity='precomputed', verbose=True)\n",
    "model.fit(distance_matrix)\n",
    "medoids = model.cluster_medoids_\n",
    "labels = model.labels_\n",
    "print('\\nMedoids :',df.columns[medoids].values)\n",
    "print('Clusters :')\n",
    "for cluster in range(n_clusters):\n",
    "    print('\\tcluster-',cluster,': ',df.columns[labels == cluster].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 混合正規分布 （確率分布に基づくクラスタリング）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実データが、なんからの確率分布がいくつか混ざった混合確率分布から生成されたものであると仮定する手法。\n",
    "\n",
    "微生物群集構造データではここ数年、Dirichlet Multinomial Mixture がよく使われている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.mixture\n",
    "import matplotlib as mpl\n",
    "\n",
    "n_samples = 500\n",
    "C = np.array([[0., -0.1], [1.7, .4]])\n",
    "X = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n",
    "          .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]\n",
    "\n",
    "gmm = sklearn.mixture.GaussianMixture(n_components=2, covariance_type='full')\n",
    "gmm.fit(X)\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "Y_ = gmm.predict(X)\n",
    "cluster_colors = ['navy', 'turquoise']\n",
    "for i, (mean, cov, color) in enumerate(zip(gmm.means_, gmm.covariances_, cluster_colors)):\n",
    "    v, w = np.linalg.eigh(cov)\n",
    "    if not np.any(Y_ == i):\n",
    "        continue\n",
    "    ax.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n",
    "    angle = np.arctan2(w[0][1], w[0][0])\n",
    "    angle = 180. * angle / np.pi\n",
    "    v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "    ell.set_clip_box(ax.bbox)\n",
    "    ell.set_alpha(.5)\n",
    "    ax.add_artist(ell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "cluster_colors = itertools.cycle(['navy', 'turquoise', 'cornflowerblue', 'darkorange', 'gray'])\n",
    "\n",
    "#  n_compoments=10 とすることで、実際の構造よりかなり大きな10個ぶんのクラスタを仮定して推論をはじめていることに注意\n",
    "# 最終的には、2つ以外のクラスタが推論の過程で勝手につぶれてくれる\n",
    "vbgmm = sklearn.mixture.BayesianGaussianMixture(n_components=10, max_iter=300,\n",
    "                                                covariance_type='full', weight_concentration_prior_type='dirichlet_distribution')\n",
    "vbgmm.fit(X)\n",
    "\n",
    "for i, cluster_weight in enumerate(vbgmm.weights_):\n",
    "    print('Cluster-', i, ' weight =', cluster_weight)\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "Y_ = vbgmm.predict(X)\n",
    "for i, (mean, cov, color) in enumerate(zip(vbgmm.means_, vbgmm.covariances_, cluster_colors)):\n",
    "    v, w = np.linalg.eigh(cov)\n",
    "    if not np.any(Y_ == i):\n",
    "        continue\n",
    "    ax.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n",
    "    angle = np.arctan2(w[0][1], w[0][0])\n",
    "    angle = 180. * angle / np.pi\n",
    "    v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "    ell.set_clip_box(ax.bbox)\n",
    "    ell.set_alpha(.5)\n",
    "    ax.add_artist(ell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# サンプル間で分散の大きいトップ1000の遺伝子のみを使って推定\n",
    "var1000_genes = std_df.var(axis=1).sort_values(ascending=False).index[:1000]\n",
    "X = std_df.loc[var1000_genes, :].transpose().values\n",
    "\n",
    "pca = sklearn.decomposition.PCA()\n",
    "pca_coords = pca.fit_transform(X)\n",
    "cluster_colors = np.array(['navy', 'turquoise', 'cornflowerblue', 'darkorange'])\n",
    "\n",
    "for n_clusters in [1, 2, 3, 4]:\n",
    "    vbgmm = sklearn.mixture.BayesianGaussianMixture(n_components=n_clusters,\n",
    "                                                    weight_concentration_prior_type='dirichlet_distribution')\n",
    "    vbgmm.fit(X)\n",
    "    print('Number of clusters estimated:', n_clusters)\n",
    "    for c in range(n_clusters):\n",
    "        print('\\tcluster-', c, ' weights =', vbgmm.weights_[c])\n",
    "    y = vbgmm.predict(X)\n",
    "    scatter_plot(pca_coords, std_df.columns, cluster_colors[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
